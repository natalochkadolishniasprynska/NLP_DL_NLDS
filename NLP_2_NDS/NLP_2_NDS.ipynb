{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8530d245-7473-4431-b91b-736ddd9593e6",
      "metadata": {
        "id": "8530d245-7473-4431-b91b-736ddd9593e6"
      },
      "source": [
        "### Plan:\n",
        "1. **Get tokens** for positive and negative tweets (by `token` in this context we mean `word`).\n",
        "2. **Lemmatize** them (convert to base word forms). For that we will use a Part-of-Speech tagger.\n",
        "3. **Clean'em up** (remove mentions, URLs, stop words).\n",
        "4. **Prepare models** for the classifier, based on cleaned-up tokens.\n",
        "5. **Run the Naive Bayes classifier**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dae761c-bd27-4d32-be19-f07965ced307",
      "metadata": {
        "id": "3dae761c-bd27-4d32-be19-f07965ced307"
      },
      "source": [
        "First, download necessary prepared samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "42819310-88ea-463b-b6a8-cd88a312c4d0",
      "metadata": {
        "id": "42819310-88ea-463b-b6a8-cd88a312c4d0"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b2ee03cc-4142-4362-85e6-508597ad4f91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2ee03cc-4142-4362-85e6-508597ad4f91",
        "outputId": "50a45178-cdfd-405c-d386-94309e49986a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca88fbc7-e208-46cc-b748-eeb3a9191fe6",
      "metadata": {
        "id": "ca88fbc7-e208-46cc-b748-eeb3a9191fe6"
      },
      "source": [
        "Get some sample positive/negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "727ac170-6412-41e6-b6c6-1038906ffbd7",
      "metadata": {
        "id": "727ac170-6412-41e6-b6c6-1038906ffbd7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import twitter_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b8c01a-fffb-42dd-bfe5-c0cf24bb7c38",
      "metadata": {
        "id": "a4b8c01a-fffb-42dd-bfe5-c0cf24bb7c38"
      },
      "source": [
        "We can either get the actual string content of those tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "25b8a199-3b39-4635-b658-375493490988",
      "metadata": {
        "id": "25b8a199-3b39-4635-b658-375493490988"
      },
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1",
        "outputId": "70ed2777-4eab-4a11-e148-96b6e08db75a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "positive_tweets[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d184a10-a0be-477b-8e9a-36749a32730a",
      "metadata": {
        "id": "9d184a10-a0be-477b-8e9a-36749a32730a"
      },
      "source": [
        "Or we can get a list of tokens using [tokenized method](https://www.nltk.org/howto/twitter.html) on `twitter_samples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e",
        "outputId": "844d53a7-2df1-445f-d0e5-8b57efffb67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
          ]
        }
      ],
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(tweet_tokens[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff367ad-5a64-4ce9-b81b-e7c9bc9103a1",
      "metadata": {
        "id": "aff367ad-5a64-4ce9-b81b-e7c9bc9103a1"
      },
      "source": [
        "Now let's setup a Part-of-Speech tagger.  Download a perceptron tagger that will be used by the PoS tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "313b4107-4729-43c4-bb72-31be6498e4ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313b4107-4729-43c4-bb72-31be6498e4ca",
        "outputId": "2dbeec27-e41e-4b4a-fc59-0973db4fc41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a170912-6686-4b4d-9e09-a6b56cb80e68",
      "metadata": {
        "id": "4a170912-6686-4b4d-9e09-a6b56cb80e68"
      },
      "source": [
        "Import Part-of-Speech tagger that will be used for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8c64174c-0358-474c-a206-8f0038f7fb35",
      "metadata": {
        "id": "8c64174c-0358-474c-a206-8f0038f7fb35"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import pos_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea756f9e-c15b-402c-9f80-bad6de2c25f3",
      "metadata": {
        "id": "ea756f9e-c15b-402c-9f80-bad6de2c25f3"
      },
      "source": [
        "Check how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4a962113-b87c-40f9-896c-a2799447d4f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a962113-b87c-40f9-896c-a2799447d4f2",
        "outputId": "72a638b1-a1dd-423e-bfce-494e2d2a9b1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('@groovinshawn', 'NN'),\n",
              " ('they', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('rechargeable', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('it', 'PRP'),\n",
              " ('normally', 'RB'),\n",
              " ('comes', 'VBZ'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('charger', 'NN'),\n",
              " ('when', 'WRB'),\n",
              " ('u', 'JJ'),\n",
              " ('buy', 'VB'),\n",
              " ('it', 'PRP'),\n",
              " (':)', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "pos_tag(tweet_tokens[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7f4c99-cb9a-40b1-a617-fe172051fa05",
      "metadata": {
        "id": "ae7f4c99-cb9a-40b1-a617-fe172051fa05"
      },
      "source": [
        "Let's write a function that will lemmatize twitter tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b70d698-6050-4bd9-b365-66ef10e6de66",
      "metadata": {
        "id": "3b70d698-6050-4bd9-b365-66ef10e6de66"
      },
      "source": [
        "For that, let's first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 [here](https://www.nltk.org/book/ch02.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "560e3e7b-a172-471c-8052-6dabbae85813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560e3e7b-a172-471c-8052-6dabbae85813",
        "outputId": "0365d29f-8f6a-4389-83b1-0a09e3863a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04a3d4c-9080-4dbc-a077-2a6e234e03c8",
      "metadata": {
        "id": "a04a3d4c-9080-4dbc-a077-2a6e234e03c8"
      },
      "source": [
        "Now fetch PoS tokens so that they can be passed to `WordNetLemmatizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842",
        "outputId": "d928215f-f92e-459c-f473-96872cd89575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "tokens = tweet_tokens[50]\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_sentence = []\n",
        "# Convert PoS tags into a format used by the lemmatizer\n",
        "# and run lemmatize\n",
        "for word, tag in pos_tag(tokens):\n",
        "    if tag.startswith('NN'):\n",
        "        pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "        pos = 'v'\n",
        "    else:\n",
        "        pos = 'a'\n",
        "    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "print(lemmatized_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba42f25-399f-40bf-a7c0-d0ff912a2f3f",
      "metadata": {
        "id": "6ba42f25-399f-40bf-a7c0-d0ff912a2f3f"
      },
      "source": [
        "Note that it converts words to their base forms ('are' -> 'be', 'comes' -> 'come').\n",
        "\n",
        "Now we can proceed to processing.\n",
        "During processing, we will perform cleanup:\n",
        "  - remove URLs and mentions using regexes\n",
        "  - after lemmatization, remove *stopwords*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "478174c9-8375-4722-88f7-8b551ed5ea59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "478174c9-8375-4722-88f7-8b551ed5ea59",
        "outputId": "69233130-497c-4e16-e60f-79b169aa1896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e47fe6-b896-4583-b971-50764e7484f9",
      "metadata": {
        "id": "40e47fe6-b896-4583-b971-50764e7484f9"
      },
      "source": [
        "What are these stopwords? Let's see some."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ac4a45d9-327b-4d06-beb1-259809529c11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4a45d9-327b-4d06-beb1-259809529c11",
        "outputId": "4277fa61-9064-4b7a-d5f6-ddf80c908b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "i\n",
            "me\n",
            "my\n",
            "myself\n",
            "we\n",
            "our\n",
            "ours\n",
            "ourselves\n",
            "you\n",
            "you're\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(len(stop_words))\n",
        "for i in range(10):\n",
        "    print(stop_words[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1bc3201-6336-4a39-9628-bf76bf612b32",
      "metadata": {
        "id": "b1bc3201-6336-4a39-9628-bf76bf612b32"
      },
      "source": [
        "Here comes the `process_tokens` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "2862fbeb-2a75-4b09-997d-1767c07e0541",
      "metadata": {
        "id": "2862fbeb-2a75-4b09-997d-1767c07e0541"
      },
      "outputs": [],
      "source": [
        "import re, string\n",
        "\n",
        "def process_tokens(tweet_tokens):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    stop_words = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub(r'#', '', token)\n",
        "\n",
        "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or\n",
        "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
        "            continue\n",
        "\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Після виконання завдання 2 (видалення знака #), точність знизилася**\n",
        "\n",
        "\n",
        "*   **Accuracy is: 0.9963333333333333**\n",
        "*   **Accuracy is: 0.9956666666666667**"
      ],
      "metadata": {
        "id": "n2j698r1mUFW"
      },
      "id": "n2j698r1mUFW"
    },
    {
      "cell_type": "markdown",
      "id": "728c7c49-46cf-4537-9fe4-750da9c38304",
      "metadata": {
        "id": "728c7c49-46cf-4537-9fe4-750da9c38304"
      },
      "source": [
        "Let's test `process_tokens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9",
        "outputId": "8430a69f-5a45-4585-edb4-9cab9b6447d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
            "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
          ]
        }
      ],
      "source": [
        "print(\"Before:\", tweet_tokens[50])\n",
        "print(\"After:\", process_tokens(tweet_tokens[50]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7880f61-72f3-4f61-b632-95df93e615dc",
      "metadata": {
        "id": "d7880f61-72f3-4f61-b632-95df93e615dc"
      },
      "source": [
        "Run `process_tokens` on all positive/negative tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9",
      "metadata": {
        "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9"
      },
      "outputs": [],
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
        "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92716ef9-e8d1-43d3-bc94-621362601f60",
      "metadata": {
        "id": "92716ef9-e8d1-43d3-bc94-621362601f60"
      },
      "source": [
        "Let's see how did the processing go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "6d90aa3b-6606-4492-901c-6f26519e926d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d90aa3b-6606-4492-901c-6f26519e926d",
        "outputId": "658bb35d-fab5-4a29-fb0f-d9216aeafdb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "['dang', 'rad', 'fanart', ':d']\n"
          ]
        }
      ],
      "source": [
        "print(positive_tweet_tokens[500])\n",
        "print(positive_cleaned_tokens_list[500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40a7964-ae39-42a5-b8da-a4a9ba9912da",
      "metadata": {
        "id": "b40a7964-ae39-42a5-b8da-a4a9ba9912da"
      },
      "source": [
        "Let's see what is most common there. Add a helper function `get_all_words`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "400ab380-4422-48bb-a513-27ca12d3ac87",
      "metadata": {
        "id": "400ab380-4422-48bb-a513-27ca12d3ac87"
      },
      "outputs": [],
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515b74f0-34db-4295-b453-cb05dda8aefa",
      "metadata": {
        "id": "515b74f0-34db-4295-b453-cb05dda8aefa"
      },
      "source": [
        "Perform frequency analysis using `FreqDist`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "05fa8805-6013-496e-8673-51308d607e1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05fa8805-6013-496e-8673-51308d607e1f",
        "outputId": "9f0258b6-c776-4363-b4dd-eb3c1601aca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 361), ('love', 336), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055ad9c7-30dd-4f83-a307-7839fe03757e",
      "metadata": {
        "id": "055ad9c7-30dd-4f83-a307-7839fe03757e"
      },
      "source": [
        "Fine. Now we'll convert these to a data structure usable for NLTK's naive Bayes classifier ([docs here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "790f1f82-d35d-45d9-90b3-52682501eb7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "790f1f82-d35d-45d9-90b3-52682501eb7d",
        "outputId": "983a10c6-46ef-4e2a-cbae-8a89097b79ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d",
      "metadata": {
        "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d"
      },
      "outputs": [],
      "source": [
        "def get_token_dict(tokens):\n",
        "    return dict([token, True] for token in tokens)\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d61171-054b-468f-bc92-e57494690f9d",
      "metadata": {
        "id": "83d61171-054b-468f-bc92-e57494690f9d"
      },
      "source": [
        "Create two datasets for positive and negative tweets. Use 7000/3000 split for train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "18964303-5c20-4a88-bcd1-df5f7e703aad",
      "metadata": {
        "id": "18964303-5c20-4a88-bcd1-df5f7e703aad"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb231b7-263f-45bb-abca-909bab161700",
      "metadata": {
        "id": "5bb231b7-263f-45bb-abca-909bab161700"
      },
      "source": [
        "Finally we use the nltk's NaiveBayesClassifier on the training data we've just created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a928308c-c221-4047-a3ff-dd56b2686c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a928308c-c221-4047-a3ff-dd56b2686c0c",
        "outputId": "6ac02324-21c0-4dbb-8eb3-ac2f843afc1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9956666666666667\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2080.0 : 1.0\n",
            "                      :) = True           Positi : Negati =    993.0 : 1.0\n",
            "                     sad = True           Negati : Positi =     25.7 : 1.0\n",
            "                      ff = True           Positi : Negati =     24.9 : 1.0\n",
            "                     bam = True           Positi : Negati =     17.6 : 1.0\n",
            "                    blog = True           Positi : Negati =     14.9 : 1.0\n",
            "                 welcome = True           Positi : Negati =     14.7 : 1.0\n",
            "               goodnight = True           Positi : Negati =     13.6 : 1.0\n",
            "               community = True           Positi : Negati =     12.9 : 1.0\n",
            "                 awesome = True           Positi : Negati =     12.5 : 1.0\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we use the Logistic Regression classifier instead**"
      ],
      "metadata": {
        "id": "xNViGz5Jl1Kl"
      },
      "id": "xNViGz5Jl1Kl"
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_classifier = SklearnClassifier(LogisticRegression())\n",
        "logistic_regression_classifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(logistic_regression_classifier, test_data))\n",
        "\n",
        "coefficients = logistic_regression_classifier._clf.coef_[0]\n",
        "feature_names = logistic_regression_classifier._vectorizer.feature_names_\n",
        "features_with_coefficients = list(zip(coefficients, feature_names))\n",
        "features_with_coefficients.sort(reverse=True)\n",
        "\n",
        "print(\"Most Informative Features:\")\n",
        "for coef, feat in features_with_coefficients[:10]:\n",
        "    print(f\"{feat} = {coef}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOROPow2iMLZ",
        "outputId": "b1625d3e-121c-4b94-d7bd-355b6fee0f3b"
      },
      "id": "jOROPow2iMLZ",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.997\n",
            "Most Informative Features:\n",
            ":) = 5.487644855817085\n",
            ":-) = 4.236041086097078\n",
            ":d = 4.177042207801741\n",
            ":p = 2.8747290530812415\n",
            ">:d = 1.277770821614759\n",
            "catch = 1.0721826145804074\n",
            "thank = 0.7230743749758483\n",
            "dots = 0.6396696845806544\n",
            "braindots = 0.6396696845806544\n",
            "brain = 0.6334698018980282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Точність Logistic Regression classifier вища за точність NaiveBayesClassifier**"
      ],
      "metadata": {
        "id": "jVHnLSWbl_cX"
      },
      "id": "jVHnLSWbl_cX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пробуємо використати крос-валідацію**"
      ],
      "metadata": {
        "id": "F0asWxdotOKf"
      },
      "id": "F0asWxdotOKf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression = LogisticRegression()\n",
        "k_fold = StratifiedKFold(n_splits=7, shuffle=True, random_state=101)\n",
        "cv_scores = cross_val_score(logistic_regression, X, y, cv=k_fold, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation of Accuracy:\", cv_scores.std())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRrC-plboZC6",
        "outputId": "48acac04-d1a1-4db7-d360-c9cb7228f9ce"
      },
      "id": "zRrC-plboZC6",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.75227432 0.74107768 0.74737579 0.7396781  0.73319328 0.73319328\n",
            " 0.73739496]\n",
            "Mean Accuracy: 0.7405981986916529\n",
            "Standard Deviation of Accuracy: 0.006582123207988753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Тут можемо зробити висновки такі (це мої suggested improvements; насправді не imrovments, а просто conclusions):**\n",
        "\n",
        "*   **може бути висока дисперсія і внаслідок цього модель буде мати меншу точність на інших даних**\n",
        "*   **можемо побачити більш правдиву оцінку моделі через усереднення результатів від кількох тестових наборів**\n"
      ],
      "metadata": {
        "id": "fVo0MKPusKCr"
      },
      "id": "fVo0MKPusKCr"
    },
    {
      "cell_type": "markdown",
      "id": "c8218b77-b2b0-4a0b-81bc-9d90cb11b154",
      "metadata": {
        "id": "c8218b77-b2b0-4a0b-81bc-9d90cb11b154"
      },
      "source": [
        "Note the Positive:Negative ratios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cdb6996-3f21-4bbd-8a71-4e3c2f1eacdf",
      "metadata": {
        "id": "0cdb6996-3f21-4bbd-8a71-4e3c2f1eacdf"
      },
      "source": [
        "Let's check some test phrase. First, download punkt sentence tokenizer ([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199",
        "outputId": "091a6480-a26a-4f71-f12d-164a36c5a865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145ea904-26bb-4f64-9ec0-634ee5f1b7bd",
      "metadata": {
        "id": "145ea904-26bb-4f64-9ec0-634ee5f1b7bd"
      },
      "source": [
        "Now we won't rely on `twitter_samples.tokenized`, but rather will use a generic tokenization routine - `word_tokenize`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "01c24fbd-1dcd-4068-9831-c56c9482a636",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01c24fbd-1dcd-4068-9831-c56c9482a636",
        "outputId": "0a1ab76b-5067-40ab-b94a-f6a7063cf8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"the service was so bad\"\n",
        "\n",
        "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(get_token_dict(custom_tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878bbbbb-7db6-4d20-8d8b-0d8c48ff79e9",
      "metadata": {
        "id": "878bbbbb-7db6-4d20-8d8b-0d8c48ff79e9"
      },
      "source": [
        "Let's package it as a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "eea00802-a825-438b-b8ef-5c36519e6fba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eea00802-a825-438b-b8ef-5c36519e6fba",
        "outputId": "cf23d58b-4fd0-46b6-8ed4-4167d4b0c8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad :  Negative\n",
            "service is bad :  Negative\n",
            "service is really bad :  Negative\n",
            "service is so terrible :  Negative\n",
            "great service :  Positive\n",
            "they stole my money :  Negative\n"
          ]
        }
      ],
      "source": [
        "def get_sentiment(text):\n",
        "    custom_tokens = process_tokens(word_tokenize(text))\n",
        "    return classifier.classify(get_token_dict(custom_tokens))\n",
        "\n",
        "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
        "for t in texts:\n",
        "    print(t, \": \", get_sentiment(t))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f574c3-ed8a-442c-9f65-bcb50551c817",
      "metadata": {
        "id": "71f574c3-ed8a-442c-9f65-bcb50551c817"
      },
      "source": [
        "Seems ok!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}